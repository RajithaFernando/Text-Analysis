{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62fa8310",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12feac47",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('response_python_4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90c70f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets = dataset['text']\n",
    "# Extracting Urls from text field of the tweet\n",
    "def getUrl(tweet):\n",
    "    x = re.search(\"(?P<url>https?://[^\\s]+)\", tweet)\n",
    "    if (x is None):\n",
    "        return None\n",
    "    else:\n",
    "        # Returning URL, removing . from beging or end of url\n",
    "        return x.group(\"url\").strip('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "b257483a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding All URLs to a list\n",
    "twitterUrls = []\n",
    "for tweet in tweets:\n",
    "    #print(tweet)\n",
    "    url = getUrl(tweet)\n",
    "    if url is not None:\n",
    "        twitterUrls.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "8066a801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Dplicate URLS and Converting to Dataframe\n",
    "twitterUrlsDF = pd.DataFrame(list(dict.fromkeys(twitterUrls)))\n",
    "# Save Urls Dataframe as CSV \n",
    "twitterUrlsDF.to_csv('URLS.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7b3f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scarping URLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f67991f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://t.co/8V1FTMxqXo'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet = 'Round 3 at St. Andrews https://t.co/8V1FTMxqXo live stream https://t.co/LCVnh3ORtf'\n",
    "url = getUrl(tweet)\n",
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "626b3574",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import urllib.request\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from bs4.element import Comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd4850ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(23, 46), match='https://t.co/8V1FTMxqXo'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2c1975f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://t.co/PX03ihgCmY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://t.co/6blkMexFws</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://t.co/SfpirDTyQX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://t.co/wktTh0nYXM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://t.co/AQ38T6Wc8S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://t.co/TEc02CDYWj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://t.co/5ZXOjh7b8b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://t.co/xVxFBketmg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://t.co/p8Ndnc14Ag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>https://t.co/9H9rZwcTHA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         0\n",
       "0  https://t.co/PX03ihgCmY\n",
       "1  https://t.co/6blkMexFws\n",
       "2  https://t.co/SfpirDTyQX\n",
       "3  https://t.co/wktTh0nYXM\n",
       "4  https://t.co/AQ38T6Wc8S\n",
       "5  https://t.co/TEc02CDYWj\n",
       "6  https://t.co/5ZXOjh7b8b\n",
       "7  https://t.co/xVxFBketmg\n",
       "8  https://t.co/p8Ndnc14Ag\n",
       "9  https://t.co/9H9rZwcTHA"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "URLS = pd.read_csv('URLS.csv')\n",
    "URLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5ededdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install contractions\n",
    "# Using contractions package \n",
    "import contractions\n",
    "def contractions_fix(text):\n",
    "    expanded_words = []   \n",
    "    for word in text.split():\n",
    "      # using contractions.fix to expand the shortened words\n",
    "      expanded_words.append(contractions.fix(word))  \n",
    "\n",
    "    expanded_text = ' '.join(expanded_words)\n",
    "    return expanded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ce131c10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Women's VNL\""
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contractions_fix(\"Women's VNL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "61cd0f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Below function will remove all tags and extract only the visibal text\n",
    "\n",
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# Below function will remove all tags and extract only the visibal text from a url\n",
    "# html = urllib.request.urlopen(url).read()\n",
    "user_agent = 'Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.0.7) Gecko/2009021910 Firefox/3.0.7'\n",
    "headers={'User-Agent':user_agent,} \n",
    "def text_from_html(url):\n",
    "    #body = urllib.request.urlopen(url, None, headers).read()\n",
    "    #soup = bs(body, 'html.parser')\n",
    "    html = requests.get(url, None).content\n",
    "    soup = bs(html, 'html.parser')\n",
    "    texts = soup.findAll(text=True)\n",
    "    visible_texts = filter(tag_visible, texts)  \n",
    "    unclean_text = u\" \".join(t.strip() for t in visible_texts)\n",
    "    \n",
    "    return unclean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fb49294d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = text_from_html('https://t.co/wktTh0nYXM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3e611290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install contractions\n",
    "# Using contractions package for contractions fix\n",
    "import contractions\n",
    "\n",
    "def contractions_fix(text):\n",
    "    expanded_words = []   \n",
    "    for word in text.split():\n",
    "      # using contractions.fix to expand the shortened words\n",
    "      expanded_words.append(contractions.fix(word))  \n",
    "\n",
    "    expanded_text = ' '.join(expanded_words)\n",
    "    return expanded_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "841bbc91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "472"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = '(from:BBCSport OR from:SportsCenter OR from:SkySportsNews OR from:TwitterSports OR sportingnews  OR from:sportingnews OR from:Sportskeeda OR from:247_SEC OR from:INQUIRERSports OR from:ESPNWWOS OR from:wwos OR from:Sportsnet OR from:es_sportsnews OR from:TheRoarSports OR from:thecoldwire OR from:WorldinSport OR from:codesportsau OR from:hssportsawards OR from:thesportsnewss OR from:Deadspin OR from:barstoolsports OR from:awfulannouncing ) lang:en has:links -is:retweet'\n",
    "len(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "199d77a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "stop_words = set(stopwords.words('english'))\n",
    "#  Referance : https://towardsdatascience.com/multi-page-document-classification-using-machine-learning-and-nlp-ba6151405c03\n",
    "\n",
    "# Function to cleen text and tokanize\n",
    "def clean_text_and_transformation(text):\n",
    "    # step 1  Case correction\n",
    "    text = text.lower()\n",
    "    print('S 1 : ',text)\n",
    "    # step 2 Removing un wanted Charactors\n",
    "\n",
    "    text_pattern = r'[?|$|&|*|%|@|(|)|~]'\n",
    "    text = re.sub(text_pattern, r'', text)\n",
    "    # step 2.2 Remove Emojis.\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                u\"\\U00002702-\\U000027B0\"\n",
    "                u\"\\U000024C2-\\U0001F251\"\n",
    "                u\"\\U0001f926-\\U0001f937\"\n",
    "                u'\\U00010000-\\U0010ffff'\n",
    "                u\"\\u200d\"\n",
    "                u\"\\u2640-\\u2642\"\n",
    "                u\"\\u2600-\\u2B55\"\n",
    "                u\"\\u23cf\"\n",
    "                u\"\\u23e9\"\n",
    "                u\"\\u231a\"\n",
    "                u\"\\u3030\"\n",
    "                u\"\\ufe0f\"\n",
    "    \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    \n",
    "    # step 3.0 Contractions before tokanization\n",
    "    text = contractions_fix(text)\n",
    "    \n",
    "    print('S 2 : ',text)\n",
    "    # step 3 Word Tokenization\n",
    "    word_tokens = word_tokenize(text)\n",
    "    # step 4 Stopwords Removal\n",
    "    filtered_text = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "    \n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7694b73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cf1ad1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd606c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf34685d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "be7894e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how to play  how to play privacy policy loading todays chuckle reload this page to see todays chuck\n",
      "javascript is not available weve detected that javascript is disabled in this browser please enable\n",
      "trending now football  soccer cricket tennis basketball other sports trending now football  soccer \n",
      "jersey vegas sponsors the best online sportsbooks and casino sites in the industry casino live casi\n",
      "skip to content english croatian czech danish dutch english esperanto french german greek hebrew hu\n",
      "nfl nfl news tampa bay buccaneers miami dolphins jacksonville jaguars college american athletic con\n",
      "home entertainment sports connect with us hi what are you looking for news247planet home entertainm\n",
      "home entertainment sports connect with us hi what are you looking for news247planet home entertainm\n",
      "home entertainment sports connect with us hi what are you looking for news247planet home entertainm\n",
      "home entertainment sports connect with us hi what are you looking for news247planet home entertainm\n"
     ]
    }
   ],
   "source": [
    "from random import random\n",
    "dirpath = 'D:\\\\Personal\\\\Msc\\\\CMM706 Text Analytics\\\\Coursework\\\\text_data_01\\\\'\n",
    "for url in URLS['0']:\n",
    "    text= text_from_html(url)\n",
    "    filename = url.split('/')[-1] + '.'+str(random()).split('.')[-1] + '.txt'\n",
    "    filepath = dirpath + filename\n",
    "    with open(filepath, 'w', encoding=\"utf-8\") as file:\n",
    "        file.write(text)\n",
    "    print(text[1:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f202bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "79183ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hashlib\n",
    "# remobing Duplicate documents by Hash Algorithom\n",
    "# Making a copy of collected data\n",
    "clean_txt_dir = \"E:\\demos\\files\\account\\\\\"\n",
    "def remove_duplicates(dirpath):\n",
    "    unique = []\n",
    "    unique_file_dir = []\n",
    "    for filename in os.listdir(dirpath):\n",
    "        if os.path.isfile(dirpath + filename):\n",
    "            # Generating a Hash for file content\n",
    "            filehash = hashlib.md5(open(dirpath + filename,'rb').read()).hexdigest()\n",
    "            if filehash not in unique: \n",
    "                unique.append(filehash)\n",
    "                \n",
    "            else: \n",
    "                os.remove(dirpath + filename)\n",
    "                print('Removing Duplicate File: ' , filename )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ab9c3d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing Duplicate File:  5ZXOjh7b8b.6279281866179528.txt\n",
      "Removing Duplicate File:  6blkMexFws.8474197935044607.txt\n",
      "Removing Duplicate File:  9H9rZwcTHA.988616841787587.txt\n",
      "Removing Duplicate File:  AQ38T6Wc8S.8892434764351882.txt\n",
      "Removing Duplicate File:  p8Ndnc14Ag.45116132295106504.txt\n",
      "Removing Duplicate File:  xVxFBketmg.5834516164606437.txt\n"
     ]
    }
   ],
   "source": [
    "remove_duplicates(dirpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ca287f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1f688d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "75f08de4",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Directory of corpus.\n",
    "\n",
    "newcorpus = PlaintextCorpusReader(dirpath, '.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "624d2f37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PlaintextCorpusReader in 'D:\\\\Personal\\\\Msc\\\\CMM706 Text Analytics\\\\Coursework\\\\text_data_01'>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newcorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "00651551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# newcorpus.fileids() Prints files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59010f19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PBS', 'debate', 'job', 'nowplaying', 'thenandnow'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mylist = ['nowplaying', 'PBS', 'PBS', 'nowplaying', 'job', 'debate', 'thenandnow']\n",
    "myset = set(mylist)\n",
    "\n",
    "myset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5985ba8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# newcorpustokens = newcorpus.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a3c38eee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "417407b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Home': 18, 'Entertainment': 36, 'Sports': 52, 'Connect': 4, 'with': 39, 'us': 10, 'Hi': 4, ',': 322, 'what': 10, 'are': 22, 'you': 62, 'looking': 4, 'for': 57, '?': 24, 'News247Planet': 8, 'New': 12, '‚Äò': 27, 'Star': 6, 'Wars': 4, '‚Äô': 162, 'Sequence': 4, 'Shonda': 4, 'Rhimes': 4, 'The': 72, 'Residence': 4, 'High': 4, 'Reveals': 4, 'Nabbing': 4, 'California': 4, 'Tax': 4, 'Credit': 4, 'Visitor': 4, 'Column': 4, ':': 162, 'I': 17, 'Had': 4, 'a': 97, 'Pre': 4, '-': 114, 'Roe': 4, 'Unlawful': 4, 'Abortion': 4, '.': 367, 'We': 11, 'Can': 4, 't': 17, 'Go': 4, 'Again': 4, 'Hollywood': 4, 'Insiders': 4, 'Mourn': 4, 'Passing': 4, 'of': 117, 'Jak': 8, 'Knight': 8, 'at': 46, '‚Äú': 30, 'A': 19, 'Painful': 4, 'Loss': 6, 'to': 165, 'Comedy': 4, '‚Äù': 17, 'Outfest': 4, 'Contained': 4, 'in': 107, 'the': 213, 'Anniversary': 4, 'L': 4, '.‚Äô': 8, 's': 98, 'LGBTQ': 4, 'Movie': 4, 'Pageant': 4, 'Stand': 4, 'Up': 4, 'Comic': 4, 'Author': 10, 'and': 159, 'Actor': 4, 'Dies': 4, 'Lakers': 16, 'Rumors': 12, 'LeBron': 4, 'James': 4, 'Russell': 8, 'Westbrook': 8, 'Summer': 4, 'season': 14, 'League': 17, 'Scene': 4, 'Was': 4, 'Tense': 4, 'Rams': 4, 'Bobby': 4, 'Wagner': 4, 'on': 71, 'Seahawks': 4, 'Launch': 4, 'Did': 4, 'not': 24, 'Need': 4, 'Depart': 4, 'Seattle': 5, 'Kyrie': 4, 'Irving': 4, 'Commerce': 8, 'Sturdy': 4, 'Perception': 4, 'Largest': 4, 'Stars': 4, 'Pushing': 4, 'Deal': 4, 'Sizzling': 4, 'Takes': 4, 'Scotty': 4, 'Pippen': 4, 'Jr': 7, 'vs': 8, 'Pelicans': 4, 'Not': 4, 'Requested': 4, 'Regardless': 4, 'Ex': 4, 'Agent': 4, 'Assertion': 4, 'Braves': 2, 'Ronald': 2, 'Acuna': 7, 'Publicizes': 1, 'He': 14, 'll': 4, 'Take': 1, 'part': 4, 'MLB': 14, 'House': 5, 'Run': 5, 'Derby': 9, 'Published': 16, 'days': 4, 'ago': 4, 'Flipboard': 8, 'Reddit': 8, 'Pinterest': 12, 'Whatsapp': 16, 'Email': 14, 'Wealthy': 1, 'von': 1, 'Biberstein': 1, '/': 38, 'Icon': 1, 'Sportswire': 1, 'through': 4, 'Getty': 3, 'Photographs': 1, 'Atlanta': 1, 'slugger': 1, 'introduced': 1, 'his': 21, 'Instagram': 1, 'story': 1, 'Monday': 6, 'that': 21, 'he': 13, 'could': 4, 'be': 37, 'taking': 1, 'this': 33, 'yr': 7, 'Dodger': 1, 'Stadium': 5, 'B': 4, 'R': 1, 'Stroll': 1, 'Off': 1, '@': 11, 'BRWalkoff': 1, 'ronaldacunajr24': 1, 'will': 30, 'take': 3, 'within': 9, 'pic': 3, 'twitter': 4, 'com': 11, 'jbASeRhB9N': 1, 'last': 1, 'participated': 1, 'reaching': 2, 'semifinals': 1, 'participant': 3, 'who': 5, 'eradicated': 1, 'end': 1, 'received': 4, 'iteration': 1, 'Pete': 2, 'Alonso': 3, 'confirmed': 1, 'participation': 1, 'occasion': 2, 'as': 36, 'effectively': 3, 'is': 56, 'two': 7, 'time': 14, 'defending': 1, 'champion': 2, 'Anthony': 1, 'DiComo': 1, 'AnthonyDiComo': 1, 'Here': 1, 'anticipation': 1, 'third': 3, 'tSQGBY78yU': 1, 'Advertisement': 17, 'Scroll': 5, 'continue': 10, 'reading': 5, 'happen': 1, 'July': 30, 'p': 1, 'm': 1, 'ET': 1, '(': 28, 'ESPN': 2, ').': 1, 'hitting': 1, 'eight': 2, 'homers': 4, 'RBI': 1, 'video': 3, 'games': 27, 'after': 7, 'recovering': 1, 'from': 19, 'torn': 1, 'ACL': 1, 'reduce': 1, 'marketing': 1, 'campaign': 1, 'quick': 6, 'had': 5, 'house': 1, 'runs': 1, 'by': 26, 'half': 1, 'final': 5, 'has': 10, 'profession': 5, 'throughout': 3, 'components': 1, 'seasons': 2, 'already': 2, 'three': 3, 'All': 7, 'clinching': 1, 'an': 24, 'automatic': 1, 'berth': 1, 'Midsummer': 1, 'Traditional': 1, 'receiving': 4, 'essentially': 1, 'most': 12, 'votes': 1, 'Nationwide': 1, 'first': 5, 'voting': 1, 'Per': 1, 'Mark': 1, 'Bowman': 1, 'hit': 1, 'toes': 2, 'or': 27, 'longer': 1, 'going': 6, 'again': 3, 'rookie': 1, 'Rockies': 1, 'C': 8, 'J': 1, 'Cron': 1, ')': 25, 'one': 23, 'trump': 1, 'whole': 1, 'inside': 1, 'span': 1, 'However': 1, 'have': 18, 'averaged': 1, 'which': 4, 'leads': 1, 'Majors': 1, 'debut': 5, '.‚Äù': 10, 'Read': 15, 'More': 21, 'In': 18, 'article': 4, 'Click': 4, 'comment': 10, 'Leave': 6, 'Reply': 18, 'Cancel': 6, 'reply': 6, 'Your': 6, 'email': 16, 'address': 6, 'published': 6, 'Required': 6, 'fields': 6, 'marked': 6, '*': 26, 'Comment': 6, 'Name': 6, 'Website': 6, 'Save': 6, 'my': 6, 'name': 6, 'website': 6, 'browser': 12, 'next': 6, 'Œî': 6, 'Trending': 8, 'Erica': 12, 'Mena': 12, 'Scores': 8, 'Her': 8, 'First': 13, 'Leading': 8, 'Role': 8, 'Starring': 8, 'Justin': 8, 'Sweat': 8, 'And': 12, 'Marques': 8, 'Houston': 8, 'Upcoming': 8, 'Thriller': 8, 'Stepmother': 12, 'Umbrella': 12, 'Academy': 16, 'Season': 8, 'Premiere': 8, 'Recap': 8, 'Apocalypse': 8, 'Quickly': 8, 'Greek': 9, 'blockchain': 12, 'specialists': 12, 'create': 8, 'genetic': 8, 'barcode': 8, 'guard': 8, 'opposition': 8, 'olive': 12, 'oil': 12, 'fraud': 8, 'You': 15, 'May': 8, 'Also': 8, 'Like': 8, 'making': 4, 'her': 4, 'acting': 4, 'upcoming': 5, 'film': 4, 'This': 12, 'reality': 4, 'TV': 4, 'star': 4, 'leading': 4, 'role': 4, '....': 8, 'sandeep335577': 16, 'June': 30, 'Photograph': 4, 'CHRISTOS': 4, 'KALOHORIDIS': 4, 'NETFLIX': 4, 'It': 16, 'been': 13, 'practically': 4, 'years': 4, 'since': 5, 'ended': 4, 'its': 7, 'second': 4, 'cheeky': 4, 'cliffhanger': 4, 'When': 6, 'our': 23, 'heroes': 4, 'returned': 4, 'current': 4, '...': 26, 'With': 10, 'funding': 8, 'EU': 4, 'analysis': 6, 'innovation': 6, 'challenge': 4, 'S3FOOD': 4, 'staff': 4, 'biotechnology': 4, 'Greece': 4, 'utilizing': 5, 'DNA': 4, 'Bop': 4, 'Store': 4, 'Songs': 4, 'From': 10, 'Lava': 4, 'La': 4, 'Rue': 4, 'Raye': 4, 'Kizz': 4, 'Daniel': 4, 'Extra': 4, 'Blackksocks': 4, 'seek': 4, 'ever': 8, 'elusive': 4, 'bop': 4, 'troublesome': 4, 'Playlists': 4, 'streaming': 4, 'service': 6, 'suggestions': 4, 'can': 40, 'solely': 4, 'achieve': 4, 'lot': 4, 'They': 9, 'usually': 4, 'depart': 4, 'lingering': 4, 'Privacy': 11, 'Policy': 10, 'Terms': 7, 'Conditions': 6, 'Contact': 12, 'Us': 10, '¬©': 13, 'Rights': 4, 'Reserved': 4, 'Posting': 4, 'JavaScript': 3, 'available': 3, 've': 1, 'detected': 1, 'disabled': 1, 'Please': 1, 'enable': 1, 'switch': 1, 'supported': 2, 'using': 1, 'see': 7, 'list': 1, 'browsers': 1, 'Help': 2, 'Center': 2, 'Service': 1, 'Cookie': 1, 'Imprint': 1, 'Ads': 1, 'info': 1, 'Twitter': 7, 'Inc': 3, 'Something': 1, 'went': 3, 'wrong': 1, 'but': 3, 'don': 5, 'fret': 1, '‚Äî': 1, 'let': 1, 'give': 6, 'it': 18, 'another': 1, 'shot': 2, 'Conor': 3, 'McGregor': 11, 'Rips': 1, 'Jake': 2, 'Paul': 8, 'After': 5, 'Battle': 1, 'Feedback': 1, 'Are': 1, 'Flop': 1, 'Child': 1, 'No': 3, 'person': 2, 'CHRISTIAN': 1, 'BRUNA': 1, 'POOL': 1, 'AFP': 1, 'Photos': 1, 'shy': 1, 'about': 5, 'want': 4, 'struggle': 5, 'boxing': 1, 'match': 3, 'however': 2, 'former': 2, 'UFC': 2, 'division': 1, 'would': 5, 'look': 1, 'like': 24, 'reiterated': 1, 'intentions': 1, 'ultimately': 1, 'whereas': 1, 'talking': 1, 'Ariel': 1, 'Helwani': 1, 'episode': 1, 'MMA': 5, 'Hour': 1, 'responded': 1, 'dismissively': 1, 'calling': 1, 'flop': 2, 'no': 5, 'TheNotoriousMMA': 1, 're': 12, 'child': 1, 'defined': 1, 'believes': 1, 'between': 1, 'him': 7, 'only': 2, 'matter': 1, 'supplied': 1, 'Irishman': 1, 'returns': 2, 'preventing': 2, 'actively': 1, 'So': 1, 'long': 4, 'continues': 2, 'successful': 1, 'methods': 1, 'mentioned': 4, 'towards': 1, 'generate': 1, 'enormous': 2, 'payday': 1, 'each': 3, 'them': 7, 'believe': 2, 'me': 3, 'pair': 1, 'extra': 2, 'instances': 1, 'persevering': 1, 'knock': 1, 'some': 1, 'individuals': 1, 'out': 6, 'get': 7, 'energetic': 1, 'once': 2, 'more': 23, 'aware': 1, 'when': 6, 'gonna': 1, 'occur': 3, 'these': 5, 'issues': 3, 'must': 1, 'sooner': 1, 'later': 1, 'gotta': 1, ',‚Äù': 4, '‚Äú,': 1, 'why': 2, 'businessman': 2, 'am': 1, '$': 19, 'million': 7, 'hasn': 1, 'fought': 1, 'misplaced': 1, 'Dustin': 1, 'Poirier': 1, 'TKO': 1, 'physician': 1, 'stoppage': 1, 'principle': 1, 'suffered': 1, 'damaged': 1, 'ankle': 1, 'closing': 1, 'moments': 1, 'primary': 2, 'spherical': 1, 'now': 6, 'fights': 2, 'along': 3, 'victory': 1, 'coming': 2, 'January': 1, 'likely': 3, 'highest': 1, 'earning': 1, 'athletes': 3, 'planet': 1, 'very': 1, 'properly': 1, 'select': 1, 'd': 1, 'nonetheless': 1, 'stay': 1, 'financially': 1, 'profitable': 1, 'due': 1, 'numerous': 1, 'ventures': 1, 'outdoors': 1, 'Octagon': 1, 'degree': 1, 'place': 3, 'decide': 1, 'curiosity': 1, 'probably': 3, 'so': 7, 'appears': 1, 'equation': 1, 'mean': 1, 'except': 1, 'does': 1, 'thing': 1, 'really': 1, 'consideration': 1, 'Skip': 1, 'content': 1, 'English': 2, 'Croatian': 1, 'Czech': 1, 'Danish': 1, 'Dutch': 1, 'Esperanto': 1, 'French': 1, 'German': 1, 'Hebrew': 1, 'Hungarian': 1, 'Irish': 1, 'Italian': 1, 'Norwegian': 1, 'Polish': 1, 'Portuguese': 1, 'Romanian': 1, 'Russian': 1, 'Slovak': 1, 'Spanish': 1, 'Swedish': 1, 'Welsh': 1, 'Random': 1, 'Posts': 4, 'Infectious': 1, 'disease': 1, 'expert': 2, 'says': 14, 'N': 4, 'S': 3, 'midst': 1, 'seventh': 1, 'COVID': 1, 'wave': 1, 'iNews': 3, 'Report': 3, 'Network': 1, 'Find': 1, 'breaking': 1, 'Internet': 2, 'news': 3, 'including': 7, 'opinion': 1, 'top': 3, 'stories': 1, 'photos': 1, 'Covid': 1, 'Topics': 1, 'Feeds': 1, 'Search': 8, 'Oilers': 4, 'Evander': 3, 'Kane': 3, 'permission': 3, 'negotiate': 1, 'other': 14, 'teams': 3, 'Edmonton': 2, 'granted': 2, 'speak': 2, 'before': 8, 'NHL': 15, \"'\": 13, 'free': 6, 'agency': 4, 'period': 2, 'opens': 2, 'Colorado': 6, 'Nicolas': 2, 'Aube': 2, 'Kubel': 2, 'celebrating': 2, 'Stanley': 6, 'Cup': 8, 'Saturday': 5, 'qualifying': 2, 'offer': 2, 'Avalanche': 9, '‚Ä¶': 5, 'posts': 1, 'Hockey': 1, 'Post': 3, 'navigation': 1, 'Man': 1, 'RCMP': 1, 'officer': 1, 'detachment': 1, 'cells': 1, 'booking': 1, 'area': 1, 'Police': 1, 'identify': 1, 'woman': 1, 'set': 1, 'fire': 1, 'TTC': 1, 'bus': 1, 'upgrade': 1, 'charge': 1, 'against': 4, 'alleged': 1, 'attacker': 1, 'Related': 1, 'Canadiens': 2, 'trade': 1, 'Petry': 2, 'Poehling': 2, 'Penguins': 3, 'exchange': 1, 'Matheson': 2, '4th': 1, 'round': 2, 'pick': 2, 'Montreal': 1, 'traded': 1, 'defenceman': 2, 'Jeff': 1, 'forward': 3, 'Ryan': 1, 'Pittsburgh': 2, 'Mike': 1, 'fourth': 2, 'Continue': 3, 'Reading': 3, 'Senators': 4, 'summer': 2, 'improvement': 1, 'signing': 4, 'Claude': 4, 'Giroux': 4, 'heading': 2, 'north': 2, 'Ottawa': 2, 'signed': 3, 'reported': 4, 'year': 6, 'US': 8, 'contract': 6, 'opened': 2, 'Wednesday': 2, 'overhaul': 1, 'Recent': 3, 'McManis': 1, 'interception': 1, 'decisive': 1, 'TD': 1, 'Argos': 1, 'rally': 1, 'past': 5, 'Roughriders': 1, 'Touchdown': 1, 'Atlantic': 9, 'game': 15, 'Contract': 2, 'workers': 1, 'found': 1, 'job': 1, 'without': 5, 'work': 1, 'permits': 1, 'Lake': 1, 'Louise': 1, 'Systemic': 1, 'change': 1, 'support': 1, 'required': 1, 'revive': 1, 'health': 2, 'care': 1, 'system': 1, 'medical': 3, 'community': 1, 'Judge': 1, 'throws': 1, 'trans': 1, 'activist': 1, 'defamation': 1, 'lawsuit': 1, 'Rebel': 1, 'News': 25, 'Doctors': 1, 'worry': 1, 'Alberta': 1, 'patients': 1, 'contrast': 1, 'dye': 1, 'shortage': 1, 'delay': 1, 'scans': 1, 'meteoblue': 1, 'USD': 1, 'EUR': 1, 'GPB': 1, 'AUD': 1, 'JPY': 1, 'YAM': 2, '%': 12, 'DSLA': 2, 'Protocol': 2, 'Lympo': 1, 'LYM': 1, 'Werewolf': 1, 'Coin': 2, 'WWC': 1, 'PolkaBridge': 1, 'PBR': 1, 'Dev': 1, 'DEV': 1, 'EvidenZ': 1, 'BCDT': 1, 'cube': 1, 'ai': 1, 'BCUBE': 1, 'Shard': 1, 'SHARD': 1, 'Finance': 1, 'Vote': 1, 'FVT': 1, 'Powered': 2, 'CoinGecko': 1, 'API': 1, 'WordPress': 1, 'Theme': 1, 'Azuma': 1, 'How': 10, 'Play': 28, 'Loading': 2, 'today': 16, 'Chuckle': 12, 'Reload': 6, 'page': 6, 'Daily': 2, 'Trivia': 2, 'Game': 4, 'Hello': 4, '!': 26, 'See': 2, 'if': 10, 'guess': 2, 'six': 4, 'tries': 2, 'less': 2, 'got': 5, 'guesses': 2, 'percentile': 2, 'among': 2, 'all': 32, 'players': 12, 'far': 4, 'Learn': 2, 'correct': 2, 'answer': 4, 'Cent': 2, 'Share': 12, 'Results': 2, 'Total': 4, 'Points': 2, '‚É£': 2, 'Were': 2, 'familiar': 2, 'Yes': 2, 'Kinda': 2, 'Attempts': 2, 'Chuck': 2, 'ü§î': 1, '‚ùå': 4, '‚úÖ': 1, 'Ô∏è‚É£': 2, '‚¨õ': 6, 'Now': 20, 'Football': 8, 'Soccer': 8, 'Cricket': 6, 'Tennis': 12, 'Basketball': 4, 'Other': 10, '28K': 4, '5K': 4, '1K': 4, 'BADMINTON': 4, 'RALLIES': 4, '|': 24, 'AMATEUR': 4, 'HIGHLIGHTS': 4, 'LOTS': 4, 'OF': 10, 'SMASHES': 4, '!!!': 5, '‚Äì': 20, 'Live': 42, 'O': 6, 'views': 12, 'minute': 6, 'read': 16, 'Shares': 4, 'Video': 24, 'Views': 6, 'On': 12, 'ENJNEERS': 4, 'Duration': 6, 'Source': 6, 'Watch': 6, 'YouTube': 5, 'guys': 3, 'complied': 2, 'badminton': 6, 'rallies': 2, 'Fortuna': 4, 'Enjoy': 8, '#': 22, 'yonex': 2, 'badmintonrallies': 2, 'badminton2022': 2, 'smash': 4, 'bestbadmintonplays': 2, 'dubai': 2, 'bestbadminton': 2, 'fitness': 2, 'Tweet': 4, 'Tags': 2, 'highlights': 4, 'Pin': 2, 'comments': 2, 'Zaif': 2, 'Sayyed': 2, 'pm': 12, 'teach': 2, 'Scatty': 4, 'Vlogs': 4, 'Amaizing': 2, 'jorelle': 2, 'gutierrez': 2, 'Naol': 2, 'pwede': 2, 'magbadminton': 2, 'VSBNY': 2, 'Cool': 2, 'harder': 2, 'DIRECTOR': 2, 'Thanks': 2, 'View': 2, 'Comments': 3, 'Round': 1, 'Gm': 1, 'Kings': 1, 'Playoffs': 1, 'Contributor': 4, 'Extended': 1, 'Kang': 1, 'Sohwi': 1, 'Sarina': 1, 'Koga': 1, 'Korea': 1, 'Japan': 1, 'Highlights': 1, 'Women': 1, 'VNL': 1, 'HD': 1, 'Titans': 1, 'Volleyball': 4, 'Fan': 1, 'F': 2, 'FPL': 2, 'FIRST': 2, 'DRAFT': 2, 'GAMEWEEK': 2, 'FANTASY': 2, 'PREMIER': 2, 'LEAGUE': 2, 'TATA': 2, 'IPL': 2, 'points': 2, 'table': 4, 'shorts': 2, 'cricket': 2, 'IPL2022': 2, 'T': 2, 'DIMITROV': 2, 'FOKINA': 2, 'Monte': 2, 'Carlo': 2, 'Masters': 2, 'LIVE': 2, 'Procommun': 2, '‚ù§‚ù§‚ù§': 1, 'FINAL': 1, 'AHMEDABAD': 1, 'VS': 1, '‚ö°Ô∏è': 1, 'KOLKATA': 1, '7pm': 1, 'RuPay': 1, 'volleyballgame': 1, 'Xvolleyball': 1, 'Hey': 1, 'Thailand': 1, '&': 1, 'Belgium': 1, 'Played': 1, 'Most': 1, 'Dramatic': 1, 'Match': 1, 'Nations': 1, 'Power': 1, 'NFL': 12, 'Tampa': 17, 'Bay': 21, 'Buccaneers': 4, 'Miami': 26, 'Dolphins': 4, 'Jacksonville': 8, 'Jaguars': 4, 'College': 6, 'American': 12, 'Athletic': 8, 'Conference': 24, 'Coast': 8, 'BIG': 8, 'Ten': 8, 'USA': 4, 'Florida': 26, 'Gators': 10, 'State': 8, 'Seminoles': 8, 'Hurricanes': 8, 'NCAA': 8, 'Pac': 4, 'ROLLINS': 4, 'SEC': 4, 'Southeast': 8, 'SOUTHEASTERN': 4, 'CONFERENCE': 8, 'SUNSHINE': 4, 'STATE': 8, 'SUNSHINES': 4, 'COMFERENCE': 4, 'UCF': 8, 'Knights': 8, 'UNIVERSITY': 4, 'TAMPA': 4, 'USF': 8, 'Bulls': 8, 'Marlins': 6, 'Rays': 8, 'York': 4, 'Yankees': 4, 'NBA': 9, 'Heat': 4, 'Orlando': 8, 'Magic': 4, 'Lightning': 11, 'Panthers': 4, 'MLS': 4, 'NASL': 4, 'Premier': 4, 'Rowdies': 4, 'Armada': 4, 'City': 4, 'SC': 4, 'Beckham': 4, 'United': 8, 'Manchester': 4, 'Lakeland': 4, 'Tropics': 4, 'Golf': 4, 'Cycling': 4, 'NASCAR': 4, 'Indy': 4, 'Car': 4, 'Horse': 8, 'Racing': 8, 'Action': 4, 'WWE': 4, 'Odds': 4, 'Radio': 8, 'Lineup': 8, 'Sign': 4, 'Welcome': 4, 'Log': 4, 'into': 9, 'your': 52, 'account': 6, 'username': 4, 'password': 14, 'Forgot': 4, 'Password': 4, 'recovery': 4, 'Recover': 4, 'Get': 10, 'help': 4, 'e': 2, 'mailed': 2, 'Talk': 6, 'Iowa': 4, 'AD': 4, 'doubts': 4, 'Big': 7, 'expands': 4, 'further': 4, 'near': 6, 'future': 4, 'Meanwhile': 4, ',‚Ä¶': 2, 'commish': 2, 'open': 5, 'business': 2, 'amid': 2, 'realignment': 2, 'talk': 2, 'Players': 2, 'Getting': 2, 'Their': 2, 'Own': 2, 'Cards': 2, 'One': 2, 'NIL': 2, 'much': 3, 'made': 3, 'Listen': 4, 'WHBO': 4, 'AM': 8, 'WWBA': 4, 'Oakland': 38, 'Elected': 4, 'Officials': 4, 'Don': 4, 'Want': 6, 'Citizen': 4, 'Opinion': 4, 'Proposed': 4, 'Village': 2, 'By': 2, 'Evan': 8, 'Weiner': 6, 'Facebook': 4, 'WhatsApp': 4, 'trying': 2, 'retain': 2, 'Major': 4, 'Baseball': 10, 'team': 2, 'Politicians': 2, 'know': 2, 'comes': 2, 'stadium': 16, 'arena': 6, 'project': 8, 'better': 2, 'put': 2, 'question': 2, 'should': 6, 'public': 12, 'go': 4, 'That': 8, 'than': 2, 'down': 7, 'overwhelming': 2, 'defeat': 2, 'politicians': 2, 'decided': 2, 'we': 8, 'need': 3, 'approval': 2, 'spend': 2, 'hundreds': 2, 'millions': 4, 'dollars': 2, 'infrastructure': 2, 'Athletics': 12, 'owner': 2, 'John': 6, 'Fisher': 14, 'countless': 2, 'tax': 4, 'breaks': 2, 'incentives': 2, 'building': 4, 'village': 4, 'waterfront': 4, 'Howard': 6, 'Terminal': 6, 'kept': 2, 'away': 2, 'reason': 3, 'citizens': 2, 'cannot': 2, 'vote': 4, 'whether': 2, 'their': 7, 'money': 2, 'private': 2, 'enterprise': 2, 'simple': 2, 'doom': 2, 'efforts': 2, 'President': 2, 'David': 2, 'Kaval': 4, 'warned': 2, 'teaching': 2, 'master': 2, 'class': 2, 'Las': 8, 'Vegas': 28, 'pitted': 2, 'threat': 2, 'moving': 2, 'worked': 2, 'San': 6, 'Francisco': 4, 'Conservation': 4, 'Development': 4, 'Commission': 4, 'voted': 2, 'remove': 2, 'port': 2, 'priority': 2, 'use': 4, 'designation': 2, 'acre': 2, 'site': 4, 'means': 3, 'proceed': 2, 'commission': 2, 'viewpoint': 2, 'referendum': 2, 'might': 7, 'binding': 2, 'served': 2, 'reminder': 2, 'people': 2, 'invest': 2, 'still': 2, 'paying': 4, 'mid': 2, '1990s': 2, 'renovation': 2, 'Coliseum': 4, 'local': 4, 'residents': 2, 'even': 6, 'eventually': 2, 'razed': 2, 'because': 2, 'there': 2, 'new': 8, 'ballpark': 4, 'wants': 2, 'also': 3, 'include': 2, 'residential': 2, 'units': 2, 'hotel': 6, 'rooms': 2, 'square': 2, 'feet': 2, 'office': 2, 'space': 2, 'seat': 2, 'theater': 2, 'acres': 2, 'parks': 2, 'books': 4, 'iTunes': 2, 'https': 2, '://': 2, 'apple': 2, 'author': 2, 'evan': 2, 'weiner': 2, 'id595575191': 2, 'reached': 2, 'evan_weiner': 2, 'hotmail': 2, 'artists': 2, 'rendering': 2, 'provided': 2, 'skyline': 2, 'view': 6, 'proposed': 2, 'Jack': 2, 'London': 2, 'Square': 2, 'Calif': 2, 'Bjarke': 2, 'Ingels': 2, 'Group': 2, 'via': 2, 'AP': 3, 'TAGS': 2, 'major': 2, 'league': 2, 'baseball': 6, 'RELATED': 2, 'ARTICLES': 2, 'MORE': 2, 'FROM': 2, 'AUTHOR': 2, 'Antonio': 2, 'Leaders': 2, 'Minor': 2, 'Park': 2, 'St': 2, 'Petersburg': 2, 'Be': 2, 'Preparing': 2, 'Proposal': 2, 'Where': 2, 'Does': 2, 'Fit': 2, 'Culture': 2, 'Fourth': 2, 'Of': 2, 'Agency': 2, 'clears': 2, 'way': 6, 'Waterfront': 2, '12B': 2, 'complex': 2, 'Happy': 2, '155th': 2, 'Birthday': 2, 'Canada': 2, 'Still': 4, 'Has': 2, 'An': 2, 'Antitrust': 2, 'Exemption': 2, 'LISTEN': 2, 'NOW': 2, 'Newsletter': 2, 'latest': 2, 'inbox': 2, 'Phone': 1, 'field': 2, 'validation': 2, 'purposes': 2, 'left': 2, 'unchanged': 2, 'Editor': 2, 'Picks': 2, 'Audio': 2, 'Coverage': 2, 'Open': 4, 'Championship': 4, 'Smith': 2, 'finishes': 2, '2nd': 2, 'Orioles': 2, 'win': 4, 'streak': 2, 'halted': 2, 'loss': 2, 'Tiger': 2, 'Woods': 2, 'gets': 2, 'emotional': 2, 'send': 2, 'off': 4, 'joined': 2, 'contenders': 2, 'being': 3, 'active': 2, ',...': 2, 'Popular': 4, 'Stories': 2, 'Interviews': 4, 'Shootout': 4, 'To': 4, 'October': 4, 'Lose': 2, 'Seeking': 2, 'Win': 2, 'Kevin': 2, 'Cash': 2, 'Postgame': 2, 'Interview': 2, 'Clinch': 2, 'AL': 2, 'East': 2, 'September': 4, 'Erik': 2, 'Neander': 2, 'Credits': 2, 'Organization': 2, 'Team': 2, 'Success': 2, 'Rock': 2, 'Riley': 2, 'Talks': 2, 'Johnny': 2, 'Townsend': 2, 'March': 2, 'ABOUT': 2, 'At': 2, 'strive': 2, 'creativity': 2, 'introduce': 2, 'followers': 2, 'distinctive': 2, 'angles': 2, 'Our': 4, 'mission': 2, 'always': 2, 'measure': 2, 'success': 2, 'growth': 2, 'development': 2, 'talent': 2, 'client': 2, 'partners': 2, 'contact': 2, 'sportstalkflorida': 2, 'FOLLOW': 2, 'Advertising': 2, 'Genesis': 2, 'Communications': 2, 'II': 5, 'IncPowered': 2, 'Assorted': 2, 'Design': 2, 'Valeri': 4, 'Nichushkin': 10, 'Conform': 1, 'Yr': 1, 'Extension': 1, 'Value': 1, 'Reported': 1, '49M': 1, 'Christian': 1, 'Petersen': 2, 'Pictures': 1, 'announced': 1, 'they': 3, 'extension': 1, 'may': 2, 'maintain': 1, 'beneath': 1, 'Sportsnet': 1, 'Elliotte': 1, 'Friedman': 1, 'earn': 1, 'averages': 1, 'yearly': 1, 'Avs': 3, 'president': 1, 'hockey': 5, 'operations': 1, 'Joe': 1, 'Sakic': 2, 'commented': 1, 'deal': 2, 'Signing': 1, 'Val': 2, 'term': 1, 'was': 5, 'high': 2, 'precedence': 1, 'offseason': 1, 'crucial': 1, 'sought': 1, 'forwards': 1, 'market': 1, 'giant': 1, 'sturdy': 1, 'tenacious': 1, 'winger': 1, 'relentless': 1, 'puck': 1, 'play': 17, 'line': 1, 'able': 1, 'transfer': 1, 'up': 7, 'lineup': 2, 'performs': 1, 'energy': 2, 'penalty': 1, 'kill': 1, 'any': 4, 'scenario': 1, 'works': 1, 'laborious': 1, 'ice': 1, 'club': 1, 'humble': 1, 'particular': 1, 'nice': 1, 'teammate': 1, 'such': 1, 'essential': 1, 'purpose': 1, 'scored': 1, 'objectives': 2, 'assisted': 1, 'others': 1, 'course': 1, 'common': 1, 'dom': 1, 'domluszczyszyn': 1, 'slight': 1, 'overpay': 1, 'feasibly': 1, 'reside': 1, 'alternative': 1, 'G8V6SP4S9F': 1, 'continued': 1, 'make': 1, 'constructive': 1, 'influence': 1, 'postseason': 1, 'ending': 1, 'factors': 2, 'assists': 1, 'playoff': 1, 'lifted': 1, 'Will': 1, 'PetersenWill': 1, 'actually': 1, 'overstated': 1, 'how': 1, 'good': 2, 'playoffs': 1, 'notably': 1, 'Ultimate': 2, 'If': 7, 'Conn': 1, 'Smythe': 1, 'just': 5, 'Finals': 1, 'MVP': 1, 'collection': 1, 'MacFarland': 1, 'GoAvsGo': 1, 'back': 2, 'titles': 1, '3rd': 1, 'straight': 7, 'efficiently': 1, 'defended': 1, 'championship': 1, 'Current': 1, 'historical': 1, 'favors': 1, 'impression': 1, 'add': 1, 'triumph': 1, 'key': 1, 'gamers': 2, 'under': 1, 'many': 5, 'notable': 1, 'exceptions': 1, 'Nazem': 1, 'Kadri': 4, 'actual': 1, 'fact': 1, 'tied': 1, 'rapidly': 1, 'improve': 1, 'probabilities': 1, 'retaining': 1, 'completed': 2, 'crew': 1, 'doable': 1, 'prioritized': 1, 'result': 1, 'grew': 1, 'fearful': 1, 'odds': 5, 'did': 1, 'wind': 1, 'dropping': 1, 'Greg': 1, 'Wyshynski': 1, 'presents': 1, 'above': 1, 'wish': 1, 'cited': 1, 'Boston': 1, 'Bruins': 1, 'Kraken': 1, 'groups': 1, 'pursue': 1, 'old': 1, 'ahead': 1, 'JERSEY': 2, 'VEGAS': 2, 'SPONSORS': 2, 'THE': 4, 'BEST': 2, 'ONLINE': 2, 'SPORTSBOOKS': 2, 'AND': 2, 'CASINO': 2, 'SITES': 2, 'IN': 2, 'INDUSTRY': 2, 'Casino': 30, 'Poker': 16, 'Sportsbook': 8, 'Racebook': 10, 'Over': 2, '+': 2, 'Style': 2, 'Online': 8, 'Games': 8, 'Best': 8, 'Casinos': 6, 'Sportsbooks': 6, 'Jersey': 8, 'sponsors': 4, 'best': 4, 'online': 10, 'sportsbooks': 2, 'casino': 24, 'sites': 8, 'industry': 2, 'largest': 6, 'selections': 8, 'Such': 6, 'Roulette': 16, 'Slots': 8, 'Blackjack': 16, 'lots': 6, 'Try': 4, 'luck': 4, 'fun': 4, 'slot': 4, 'machines': 4, 'fond': 6, 'classic': 6, 'Baccarat': 8, 'then': 6, 'find': 12, 'here': 9, 'easy': 4, 'safe': 4, 'directly': 4, 'endless': 6, 'variety': 6, 'sports': 6, 'leagues': 2, 'around': 2, 'world': 16, 'bet': 2, 'Started': 4, 'roulette': 4, 'blackjack': 4, 'chatting': 4, 'fellow': 4, 'live': 10, 'dealers': 8, 'hours': 4, 'day': 4, 'action': 8, 'unfolds': 4, 'eyes': 4, 'closest': 4, 'true': 4, 'experience': 4, 'airfare': 4, 'cost': 4, 'Experience': 4, 'Dealer': 4, 'Sit': 4, 'greats': 4, 'Texas': 5, 'Hold': 4, 'em': 4, 'Omaha': 4, 'lightning': 4, 'speed': 4, 'fastest': 4, 'poker': 12, 'mix': 4, 'variants': 4, 'Whatever': 4, 'thousands': 4, 'over': 6, 'Boxing': 4, 'handball': 4, 'formula': 4, 'football': 6, 'sample': 4, 'There': 4, 'amount': 4, 'choices': 4, 'wager': 4, 'types': 4, 'bets': 10, 'parlays': 4, 'teasers': 4, 'pleasers': 4, 'monsters': 4, 'Sponsored': 4, 'Sites': 6, 'lucky99': 2, 'eu': 10, 'Lucky99': 10, 'America': 2, 'entertainment': 2, 'destination': 2, 'Player': 2, 'approach': 2, 'betting': 14, 'style': 2, 'makes': 2, 'winner': 2, 'soon': 2, 'BigDaddy': 4, 'Lotto': 4, 'U': 2, 'Pick': 10, 'Exclusive': 2, 'lottery': 6, 'offers': 2, 'corner': 2, 'store': 2, 'come': 2, 'close': 2, 'matching': 2, 'spot': 2, 'tickets': 2, 'state': 2, 'lotteries': 2, 'higher': 2, 'payouts': 2, 'Worldwide': 2, 'accepted': 2, 'featured': 2, 'Join': 12, 'feel': 2, 'floor': 2, 'expanding': 2, 'choice': 2, 'favourite': 2, 'professional': 4, 'meet': 2, 'entertaining': 2, '!.': 2, 'Choose': 2, 'wide': 4, 'favorite': 2, 'Single': 2, 'Zero': 2, 'Enhanced': 2, 'Payout': 4, '),': 2, 'Early': 2, '‚Ñ¢.': 1, 'greatest': 2, 'beautiful': 2, 'sponsor': 2, 'shares': 4, 'reliable': 2, 'secure': 2, 'provide': 2, 'satisfied': 2, 'customers': 2, 'Bet': 2, 'biggest': 2, 'sporting': 2, 'events': 2, 'Rolling': 2, 'wagering': 2, 'applied': 2, 'pending': 2, 'plays': 4, 'premier': 2, 'Betting': 2, 'Destination': 2, 'thoroughbred': 2, 'harness': 2, 'quarter': 2, 'horse': 8, 'tracks': 2, 'access': 2, 'performances': 2, 'advanced': 2, 'platform': 2, 'where': 2, 'watch': 2, 'races': 2, 'Whether': 2, 'padding': 2, 'bankroll': 2, 'Triple': 2, 'Crown': 2, 'require': 2, 'Wager': 2, 'Kentucky': 2, 'Preakness': 2, 'Stakes': 4, 'Belmont': 2, 'Breeders': 2, ';': 2, 'track': 2, 'lines': 2, 'net': 2, 'As': 2, 'well': 2, 'having': 2, 'racing': 2, 'racetracks': 2, 'across': 3, 'everything': 2, 'show': 2, 'exotic': 2, 'wagers': 2, 'exactas': 2, 'trifectas': 2, 'superfectas': 2, 'Playing': 2, 'horses': 2, 'similar': 2, 'playing': 2, 'options': 2, 'boxed': 2, 'numerical': 2, 'About': 2, 'Table': 2, 'Recommend': 2, 'BetAnySports': 4, 'Links': 2, 'Responsible': 2, 'Gaming': 2, 'JerseyVegas': 2, 'proudly': 2, '‚Ñ¢Ô∏è.': 1, 'Former': 1, 'Cowboys': 2, 'Bears': 2, 'RB': 1, 'Marion': 5, 'Barber': 6, 'III': 2, 'Died': 1, 'Warmth': 1, 'Stroke': 1, 'per': 1, 'Medical': 1, 'Examiner': 1, 'Picture': 1, 'Cody': 1, 'Responsibility': 1, 'Collin': 1, 'County': 1, 'workplace': 1, 'dominated': 1, 'operating': 1, 'died': 1, 'results': 1, 'warmth': 1, 'stroke': 1, 'response': 1, 'TMZ': 1, 'Officers': 1, 'additionally': 2, 'deemed': 1, 'dying': 2, 'accident': 1, 'performed': 1, 'Dallas': 2, 'Chicago': 1, 'discovered': 2, 'useless': 1, 'Frisco': 1, 'police': 2, 'father': 1, 'informed': 1, 'Clarence': 1, 'E': 1, 'Hill': 1, 'Fort': 1, 'Price': 1, 'Telegram': 1, 'son': 1, 'physique': 2, 'badly': 1, 'decomposed': 1, 'point': 1, 'simply': 2, 'tissue': 1, 'dig': 1, 'additional': 1, 'ruling': 2, 'seen': 1, 'trauma': 1, 'foul': 1, 'substances': 1, 'His': 1, 'lungs': 1, 'working': 1, 'order': 1, 'center': 1, 'veins': 1, 'coronary': 1, 'heart': 1, 'Proper': 1, 'ready': 1, 'household': 1, 'donate': 1, 'mind': 1, 'functions': 1, 'primarily': 1, 'based': 1, 'acknowledged': 1, 'spent': 1, 'seven': 1, 'Professional': 1, 'Bowl': 1, 'dashing': 1, 'yards': 2, 'touchdowns': 1}\n"
     ]
    }
   ],
   "source": [
    "# Ref : https://lost-contact.mit.edu/afs/cs.pitt.edu/projects/nltk/docs/tutorial/introduction/x546.html\n",
    "words = newcorpus.words()\n",
    "fdist1 = FreqDist(words)\n",
    "\n",
    "filtered_word_freq = dict((word, freq) for word, freq in fdist1.items() if not word.isdigit())\n",
    "\n",
    "# print(filtered_word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6911167d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bad8aede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Words :  1955\n",
      "Total Words :  1955\n"
     ]
    }
   ],
   "source": [
    "# words = nltk.tokenize.word_tokenize(text1)\n",
    "unique_words = len(filtered_word_freq)\n",
    "print('Unique Words : ', unique_words)\n",
    "total_words = len(newcorpus.words())\n",
    "print('Total Words : ', unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "80635911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# data = filtered_word_freq\n",
    "# names = list(data.keys())\n",
    "# values = list(data.values())\n",
    "\n",
    "# plt.bar(range(len(data)), values, tick_label=names)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "06a9ec6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "def bow_extractor(corpus, ngram_range=(1,1)):\n",
    "    \n",
    "    vectorizer = CountVectorizer(min_df=1, ngram_range=ngram_range)\n",
    "    features = vectorizer.fit_transform(corpus)\n",
    "    return vectorizer, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f539232f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_features(features, feature_names):\n",
    "    df = pd.DataFrame(data=features,\n",
    "                      columns=feature_names)\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8244a1fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'PlaintextCorpusReader' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [53]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m bow_vectorizer, bow_features \u001b[38;5;241m=\u001b[39m \u001b[43mbow_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnewcorpus\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [51]\u001b[0m, in \u001b[0;36mbow_extractor\u001b[1;34m(corpus, ngram_range)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbow_extractor\u001b[39m(corpus, ngram_range\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m      4\u001b[0m     vectorizer \u001b[38;5;241m=\u001b[39m CountVectorizer(min_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, ngram_range\u001b[38;5;241m=\u001b[39mngram_range)\n\u001b[1;32m----> 5\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m vectorizer, features\n",
      "File \u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1330\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1322\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1323\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1324\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1325\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1326\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1327\u001b[0m             )\n\u001b[0;32m   1328\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1330\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1332\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1333\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1199\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1197\u001b[0m values \u001b[38;5;241m=\u001b[39m _make_int_array()\n\u001b[0;32m   1198\u001b[0m indptr\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m-> 1199\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[0;32m   1200\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m   1201\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m analyze(doc):\n",
      "\u001b[1;31mTypeError\u001b[0m: 'PlaintextCorpusReader' object is not iterable"
     ]
    }
   ],
   "source": [
    "bow_vectorizer, bow_features = bow_extractor(newcorpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582d7695",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8665ce71",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [55]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnewcorpus\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m fdist1 \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mFreqDist(words)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# tokens = nltk.word_tokenize(newcorpus)\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    132\u001b[0m     ]\n",
      "File \u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py:107\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;124;03mReturn a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03musing NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m:param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    106\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m load(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlanguage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1276\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.tokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize\u001b[39m(\u001b[38;5;28mself\u001b[39m, text, realign_boundaries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   1273\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1274\u001b[0m \u001b[38;5;124;03m    Given a text, returns a list of the sentences in that text.\u001b[39;00m\n\u001b[0;32m   1275\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentences_from_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrealign_boundaries\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1332\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.sentences_from_text\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msentences_from_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, text, realign_boundaries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1327\u001b[0m \u001b[38;5;124;03m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[0;32m   1328\u001b[0m \u001b[38;5;124;03m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[0;32m   1329\u001b[0m \u001b[38;5;124;03m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[0;32m   1330\u001b[0m \u001b[38;5;124;03m    follows the period.\u001b[39;00m\n\u001b[0;32m   1331\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1332\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [text[s:e] \u001b[38;5;28;01mfor\u001b[39;00m s, e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1332\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msentences_from_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, text, realign_boundaries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1327\u001b[0m \u001b[38;5;124;03m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[0;32m   1328\u001b[0m \u001b[38;5;124;03m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[0;32m   1329\u001b[0m \u001b[38;5;124;03m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[0;32m   1330\u001b[0m \u001b[38;5;124;03m    follows the period.\u001b[39;00m\n\u001b[0;32m   1331\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1332\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [text[s:e] \u001b[38;5;28;01mfor\u001b[39;00m s, e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1322\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.span_tokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m realign_boundaries:\n\u001b[0;32m   1321\u001b[0m     slices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_realign_boundaries(text, slices)\n\u001b[1;32m-> 1322\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m slices:\n\u001b[0;32m   1323\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m (sentence\u001b[38;5;241m.\u001b[39mstart, sentence\u001b[38;5;241m.\u001b[39mstop)\n",
      "File \u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1421\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._realign_boundaries\u001b[1;34m(self, text, slices)\u001b[0m\n\u001b[0;32m   1408\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1409\u001b[0m \u001b[38;5;124;03mAttempts to realign punctuation that falls after the period but\u001b[39;00m\n\u001b[0;32m   1410\u001b[0m \u001b[38;5;124;03mshould otherwise be included in the same sentence.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1418\u001b[0m \u001b[38;5;124;03m    [\"(Sent1.)\", \"Sent2.\"].\u001b[39;00m\n\u001b[0;32m   1419\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1420\u001b[0m realign \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1421\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence1, sentence2 \u001b[38;5;129;01min\u001b[39;00m _pair_iter(slices):\n\u001b[0;32m   1422\u001b[0m     sentence1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mslice\u001b[39m(sentence1\u001b[38;5;241m.\u001b[39mstart \u001b[38;5;241m+\u001b[39m realign, sentence1\u001b[38;5;241m.\u001b[39mstop)\n\u001b[0;32m   1423\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sentence2:\n",
      "File \u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py:318\u001b[0m, in \u001b[0;36m_pair_iter\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    316\u001b[0m iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(iterator)\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 318\u001b[0m     prev \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1395\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._slices_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1393\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_slices_from_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[0;32m   1394\u001b[0m     last_break \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1395\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m match, context \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_match_potential_end_contexts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1396\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_contains_sentbreak(context):\n\u001b[0;32m   1397\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mslice\u001b[39m(last_break, match\u001b[38;5;241m.\u001b[39mend())\n",
      "File \u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1375\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._match_potential_end_contexts\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1373\u001b[0m before_words \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m   1374\u001b[0m matches \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m-> 1375\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m match \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lang_vars\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperiod_context_re\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinditer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m)):\n\u001b[0;32m   1376\u001b[0m     \u001b[38;5;66;03m# Ignore matches that have already been captured by matches to the right of this match\u001b[39;00m\n\u001b[0;32m   1377\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m matches \u001b[38;5;129;01mand\u001b[39;00m match\u001b[38;5;241m.\u001b[39mend() \u001b[38;5;241m>\u001b[39m before_start:\n\u001b[0;32m   1378\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(newcorpus.sents())\n",
    "fdist1 = nltk.FreqDist(words)\n",
    "# tokens = nltk.word_tokenize(newcorpus)\n",
    "\n",
    "filtered_word_freq = dict((word, freq) for word, freq in fdist1.items() if not word.isdigit())\n",
    "\n",
    "print(filtered_word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8cbe7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8dc9fa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import urllib.request\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from bs4.element import Comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "87730870",
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://t.co/9H9rZwcTHA'\n",
    "html = requests.get(url).content\n",
    "soup = bs(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c438360d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = soup.findAll(text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5589fe74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "42a5a485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below function will remove all tags and extract only the visibal text from a url\n",
    "# html = urllib.request.urlopen(url).read()\n",
    "def text_from_html(url):\n",
    "    body = urllib.request.urlopen(url).read()\n",
    "    soup = bs(body, 'html.parser')\n",
    "    texts = soup.findAll(text=True)\n",
    "    visible_texts = filter(tag_visible, texts)  \n",
    "    return u\" \".join(t.strip() for t in visible_texts)\n",
    "def clear_character_text(textset):\n",
    "    textset.replace('\"', \"\")\n",
    "    # Removing Spaces (Double Spaces , Tabs, etc)\n",
    "    textset = re.sub(' +', ' ',textset)\n",
    "    \n",
    "    return textset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "c8fbf3e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'                     Home  Entertainment  Sports      Connect with us          Hi, what are you looking for?                                       News247Planet     Home  Entertainment Simply 43 Merchandise That TikTokers Assume Are Price Splurging On 17 Actors That Proved Wigs Can Actually Make All The Distinction In TV And Films ‚ÄúMarried At First Sight‚Äù Season 15 Is Right here, And These Are The 5 {Couples} Who Are Getting Hitched 30 Tops You may Put on All Summer season Lengthy Millennials And Gen Z‚Äôers, I Am Genuinely Curious If You Like Or Have Even Heard These 45 Basic Different ‚Äô90s Songs  Sports Aston Martin F1 : Une affaire de racisme et d‚Äôhomophobie fait floor Aston Martin F1 : Krack ‚Äòesp√®re‚Äô que Vettel restera apr√®s 2022 McLaren F1 : Ricciardo ne veut ‚Äòrien laisser au hasard‚Äô pour comprendre ses difficult√©s Perez se f√©licite d‚Äôune relation saine et sans jalousie avec Verstappen Hamilton √† Leclerc : P***** je ne voulais pas t‚Äôaccrocher et t‚Äôenvoyer dehors √† Copse !                            Sports   Conor McGregor Rips Jake Paul After Battle Feedback: ‚ÄòYou Are a Flop, Child. A No person‚Äô      Published  4 days ago                                Flipboard       Reddit       Pinterest       Whatsapp       Whatsapp       Email                                   CHRISTIAN BRUNA/POOL/AFP through Getty Photos   Jake Paul has not been shy about his want to struggle Conor McGregor in a boxing match, however the former UFC two-division champion would not look like . Paul reiterated his intentions to ultimately struggle McGregor whereas talking to Ariel Helwani on Monday‚Äôs episode of The MMA Hour . McGregor responded on Twitter dismissively, calling Paul ‚Äúa flop‚Äù and ‚Äúa no one.‚Äù   Conor McGregor @TheNotoriousMMA  You‚Äôre a flop, child. A no one.    Paul defined that he believes a struggle between him and McGregor is only a matter of time, supplied that the Irishman returns to preventing actively. So long as Paul continues his successful methods, he mentioned a struggle towards McGregor would generate an enormous payday for each of them. ‚ÄúI believe for me, preventing a pair extra instances, persevering with to knock some individuals out, and he has to get energetic once more, who is aware of when that is gonna occur? I believe these two issues must occur, after which sooner or later, it is gotta occur,‚Äù Paul mentioned. ‚Äú, why not? He is a businessman, I am a businessman, and that is $75 to $100 million for each of us.‚Äù McGregor hasn‚Äôt fought since final July when he misplaced to Dustin Poirier by TKO (physician stoppage) in the principle occasion of UFC 264. McGregor had suffered a damaged ankle within the closing moments of the primary spherical. He is now 1-3 in his final 4 fights, along with his final victory coming in January 2020. Advertisement. Scroll to continue reading.    McGregor is likely one of the highest-earning athletes on the planet at this time and will very properly select to not struggle once more, and he‚Äôd nonetheless stay financially profitable due to his numerous ventures outdoors of the Octagon. He is at a degree in his profession the place he can decide the fights that curiosity him probably the most, so Paul appears to be out of that equation in the mean time except he does one thing to really get McGregor‚Äôs consideration.   Read More        In this article:             Click to comment     Leave a Reply  Cancel reply Your email address will not be published.  Required fields are marked * Comment *  Name *   Email *   Website    Save my name, email, and website in this browser for the next time I comment.     Œî            Advertisement       Trending           Entertainment    Erica Mena Scores Her First Leading Role Starring Justin Sweat And Marques Houston In Upcoming Thriller, ‚ÄúThe Stepmother‚Äù               Entertainment    The Umbrella Academy\\xa0Season-Premiere Recap: Apocalypse Quickly               Entertainment    Greek blockchain specialists create genetic barcode to guard in opposition to olive oil fraud          Advertisement                                      Flipboard       Reddit       Pinterest       Whatsapp       Whatsapp       Email               You May Also Like               Entertainment    Erica Mena Scores Her First Leading Role Starring Justin Sweat And Marques Houston In Upcoming Thriller, ‚ÄúThe Stepmother‚Äù    Erica Mena will be making her acting debut in an upcoming film, ‚ÄúThe Stepmother.‚Äù This will be the reality TV star‚Äôs first leading role....   sandeep335577 June 17, 2022                 Entertainment    The Umbrella Academy\\xa0Season-Premiere Recap: Apocalypse Quickly    Photograph: CHRISTOS KALOHORIDIS/NETFLIX It‚Äôs been practically two years since\\xa0The Umbrella Academy\\xa0ended its second season\\xa0on a cheeky cliffhanger. When our heroes returned to the current...   sandeep335577 June 22, 2022                 Entertainment    Greek blockchain specialists create genetic barcode to guard in opposition to olive oil fraud    With funding from the EU analysis and innovation challenge S3FOOD, a staff of biotechnology and blockchain specialists in Greece is utilizing olive oil DNA...   sandeep335577 June 28, 2022                 Entertainment    Bop Store: Songs From Lava La Rue, Raye, Kizz Daniel, And Extra    Blackksocks The seek for the ever-elusive ‚Äúbop‚Äù is troublesome. Playlists and streaming-service suggestions can solely achieve this a lot. They usually depart a lingering...   sandeep335577 July 2, 2022            Advertisement                   Home  Privacy Policy  Terms and Conditions  Contact Us     ¬© 2022 News247Planet. All Rights Reserved                          Posting....                           '"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca2e60b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "158db551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'                    Home  Entertainment  Sports      Connect with us          Hi, what are you look'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mytext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c941e2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mytext = text[1:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f2f2cb2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'                    Home  Entertainment  Sports      Connect with us          Hi, what are you look'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mytext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "955d6aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "newText =  clear_character_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6a6eacf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a93917d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_text_to_csv(textline):\n",
    "#     print(textline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "807e2628",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1b837ea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Home Entertainment Sports Connect with us Hi, what are you look'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add01b6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
