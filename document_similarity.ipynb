{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fcfa19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from normalization import normalize_corpus\n",
    "# from utils import build_feature_matrix\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bc9c358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define a toy corpus (collection of documents) to explore the ideas\n",
    "toy_corpus = ['The sky is blue',\n",
    "'The sky is blue and beautiful',\n",
    "'Look at the bright blue sky!',\n",
    "'Python is a great Programming language',\n",
    "'Python and Java are popular Programming languages',\n",
    "'Among Programming languages, both Python and Java are the most used in Analytics',\n",
    "'The fox is quicker than the lazy dog',\n",
    "'The dog is smarter than the fox',\n",
    "'The dog, fox and cat are good friends']\n",
    "\n",
    "# Documents that we will be measuring similarities for\n",
    "query_docs = ['The fox is definitely smarter than the dog',\n",
    "            'Java is a static typed programming language unlike Python',\n",
    "            'I love to relax under the beautiful blue sky!']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8c3c781",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_corpus(corpus, lemmatize=True, only_text_chars=False, tokenize=False):\n",
    "    \n",
    "    normalized_corpus = []    \n",
    "    for text in corpus:\n",
    "        text = text.lower()\n",
    "        normalized_corpus.append(text)\n",
    "            \n",
    "    return normalized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "215122a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We improve our feature matrix builder with 3 additional optional parameters\n",
    "# This allows us to extract not only word features, but also n-gram features\n",
    "# We can also set the minimum and maximum frequencies to be considered as valid\n",
    "# NB: All these are simply passed on to sklearn's Vectorizer classes\n",
    "def build_feature_matrix(documents, feature_type='frequency',\n",
    "                         ngram_range=(1, 1), min_df=0.0, max_df=1.0):\n",
    "\n",
    "    feature_type = feature_type.lower().strip()  \n",
    "    \n",
    "    if feature_type == 'binary':\n",
    "        vectorizer = CountVectorizer(binary=True, min_df=min_df,\n",
    "                                     max_df=max_df, ngram_range=ngram_range)\n",
    "    elif feature_type == 'frequency':\n",
    "        vectorizer = CountVectorizer(binary=False, min_df=min_df,\n",
    "                                     max_df=max_df, ngram_range=ngram_range)\n",
    "    elif feature_type == 'tfidf':\n",
    "        vectorizer = TfidfVectorizer(min_df=min_df, max_df=max_df, \n",
    "                                     ngram_range=ngram_range)\n",
    "    else:\n",
    "        raise Exception(\"Wrong feature type entered. Possible values: 'binary', 'frequency', 'tfidf'\")\n",
    "\n",
    "    feature_matrix = vectorizer.fit_transform(documents).astype(float)\n",
    "    \n",
    "    return vectorizer, feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "600b1862",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cosine_similarity(doc_features, corpus_features,\n",
    "                              top_n=3):\n",
    "    # Get document vectors\n",
    "    doc_features = doc_features[0]\n",
    "    # Compute similarities by calling dot.product on transposed corpus feature vector\n",
    "    similarity = np.dot(doc_features, \n",
    "                        corpus_features.T)\n",
    "    similarity = similarity.toarray()[0]\n",
    "    # Get docs with highest similarity scores\n",
    "    top_docs = similarity.argsort()[::-1][:top_n]\n",
    "    top_docs_with_score = [(index, round(similarity[index], 3))\n",
    "                            for index in top_docs]\n",
    "    return top_docs_with_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7c2e3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_corpus = normalize_corpus(toy_corpus, lemmatize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12b789fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the sky is blue',\n",
       " 'the sky is blue and beautiful',\n",
       " 'look at the bright blue sky!',\n",
       " 'python is a great programming language',\n",
       " 'python and java are popular programming languages',\n",
       " 'among programming languages, both python and java are the most used in analytics',\n",
       " 'the fox is quicker than the lazy dog',\n",
       " 'the dog is smarter than the fox',\n",
       " 'the dog, fox and cat are good friends']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bc9ede0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer, tfidf_features = build_feature_matrix(norm_corpus,\n",
    "                                                        feature_type='tfidf',\n",
    "                                                        ngram_range=(1, 1), \n",
    "                                                        min_df=0.0, max_df=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02f4a8b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e28960dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1 : The fox is definitely smarter than the dog\n",
      "Top 2 similar docs:\n",
      "----------------------------------------\n",
      "Doc num: 8 Similarity Score: 1.0\n",
      "Doc: The dog is smarter than the fox\n",
      "----------------------------------------\n",
      "Doc num: 7 Similarity Score: 0.671\n",
      "Doc: The fox is quicker than the lazy dog\n",
      "----------------------------------------\n",
      "\n",
      "Document 2 : Java is a static typed programming language unlike Python\n",
      "Top 2 similar docs:\n",
      "----------------------------------------\n",
      "Doc num: 4 Similarity Score: 0.739\n",
      "Doc: Python is a great Programming language\n",
      "----------------------------------------\n",
      "Doc num: 5 Similarity Score: 0.48\n",
      "Doc: Python and Java are popular Programming languages\n",
      "----------------------------------------\n",
      "\n",
      "Document 3 : I love to relax under the beautiful blue sky!\n",
      "Top 2 similar docs:\n",
      "----------------------------------------\n",
      "Doc num: 2 Similarity Score: 0.867\n",
      "Doc: The sky is blue and beautiful\n",
      "----------------------------------------\n",
      "Doc num: 1 Similarity Score: 0.67\n",
      "Doc: The sky is blue\n",
      "----------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Similarly, we normalize and extract features from the query corpus\n",
    "norm_query_docs =  normalize_corpus(query_docs, lemmatize=True)   \n",
    "# We use the same vectorizer that we used to build the feature matrix for the corpus also for query doc         \n",
    "query_docs_tfidf = tfidf_vectorizer.transform(norm_query_docs)\n",
    "\n",
    "for index, doc in enumerate(query_docs):\n",
    "    \n",
    "    doc_tfidf = query_docs_tfidf[index]\n",
    "    top_similar_docs = compute_cosine_similarity(doc_tfidf,\n",
    "                                             tfidf_features,\n",
    "                                             top_n=2)\n",
    "    print('Document',index+1 ,':', doc)\n",
    "    print('Top', len(top_similar_docs), 'similar docs:')\n",
    "    print('-'*40)\n",
    "    for doc_index, sim_score in top_similar_docs:\n",
    "        print('Doc num: {} Similarity Score: {}\\nDoc: {}'.format(doc_index+1,\n",
    "                                                                 sim_score,\n",
    "                                                                 toy_corpus[doc_index]))\n",
    "        print('-'*40)    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f493d1cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
